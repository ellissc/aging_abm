---
title: "Aging and Language Change"
author: "Ellis Cain"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
library(wesanderson)
library(patchwork)
library(ggdist)
knitr::opts_knit$set(root.dir = here())
```


\newcommand\latexcode[1]{#1}

# Abstract

Language change can be understood as representing the dynamic relationship between group-level and individual usage patterns. 
As statistical learners who are sensitive to the underlying distributional properties of language, when usage patterns vary across time, it would be expected that the language acquisition of different age groups will be impacted by the usage patterns they experience.
While previous research has shown differences in the language representation and cognitive capabilities over the lifespan (i.e., early vocab spurt, late cognitive decline), the impact of varying learning rates on language change is not well understood.
Using an agent-based model of (syntactic) language change, this project aims to explore the interaction between learning rates and aging may impact diachronic changes in usage patterns of language.



# Introduction

## Language change

<!-- Languages change over historical time~\cite<e.g.,>{bybee2015language} in terms of their usage statistics across users \cite{michelQuantitativeAnalysisCulture2011}. Yet, little is known about how these changes impact the meaning representations of an individual language user.  -->
<!-- The organization of lexico-semantic representations across the lifespan has been studied using word association data~\cite<e.g.,>{dedeyneSmallWorldWords2019}.  -->
<!-- Dubossarsky and colleagues \citeyear{dubossarskyQuantifyingStructureFree2017} analyzed lexical association networks and found that their organization changes over time. Namely, they observed a U-shaped developmental trajectory: networks are small and sparse (e.g., reduction of average shortest path length and entropy) during language acquisition (10--18 y.o.), they are dense and well-connected in mid-life (e.g., increased in-degree and out-degree), and then they become sparse again, with a larger proportion of isolated, peripheral nodes, in late-life (e.g., reduced in/out degree, increased entropy).  -->

## Verb bias example

<!-- Verb bias stuff w/ Rachel  -->

## Aging

<!-- In addition to age-related differences in lexico-semantic organization, previous studies have used electroencephalography (EEG) to explore the relationship between lexico-semantic processing and aging~\cite<e.g.,>{federmeier2010age, wlotkoPredictNotPredict2012}. Some signatures of predictive processing (e.g., increased anterior positivity in response to a word that is less closely related to a preceding cue) are reduced in older adults, suggesting that aging may be related to changes in neural mechanisms of prediction and/or in the predictability relationships among words and meanings. -->

## Previous models of language change

Beeksma et al., 2017

Troutman et al., ?



# Model

## Overview

I aim to explore how potential changes in learning rates across the lifespan may impact overall population-level patterns of language change. 
The model will explore the usage and spread of a grammatical variant throughout a population.
For example, the past tense ending has some variation, either appearing as "-t" or "-ed", such as in "learnt" or "learned". [**verb bias example**]
*To define: learn / learning rates*
The following main assumptions are based on those found in Beeksma et al. and Troutman et al.:

1. Language learning is based on imitating others, though this may change over the lifespan.

    - E.g., individuals may learn quickly early on, but slow down as they age.

2. There are variations in preference between individuals.

    - E.g., some individuals learn more quickly than others.

3. Language can be influenced by external factors.

    - E.g., more willing to learn from in-group members, natural bias towards a given variant.

With this model, the main outcome patterns are as follows (adapted from Troutman et al.):

1. S-shaped curve in usage patterns: Change happens slowly, then proceeds rapidly before slowing down again.

2. Intra-speaker variation: Change is gradual and there is a period of intra-speaker variation.

3. Categorical norms: With competition, speakers move toward categorically using just one of the competing variants.

4. Multi-stability: Language change can have multiple stable outcomes. **Additionally, some variation in language usage may co-exist. (Verb bias example)**

5. Threshold problem: Initially rare variants may manage to spread through entire speech communities.

## Model description

This model is based on the language change model from Troutman et al., implemented in netlogo.

The model consists of a preferential attachment network, where each node represents a language user. There are two grammar variants, which are coded as 0 or 1.

Nodes have the following properties; state, original state, spoken state, age, gamma, and cohort.
State represents the node's current grammar state, ranging from [0,1], which will influence their likelihood to use one of the two grammar variants. It can be understood as the bias towards a specific grammar (e.g., node with state of 0.25 would be biased towards Grammar 0).
Original state represents their initially assigned grammar state.
Spoken state represents the output of each node's speech, as (grammar) 0 or 1.
The speaking process will be detailed in the *dynamics* section.
Each node also has an age, that increases by 1 each tick, and a gamma value, which serves as their learning rate (step size).
Higher gamma values (0.04) mean that the node will adjust their state more when listening, as opposed to a lower gamma (0.01).
Each node also has a cohort or age group, which is either 1 or 2. This is fixed from the start.

For initialization, the model uses [**what implementation**] to generate the preferential attachment network with a specified number of nodes. **Comment from Troutman paper?**
Grammar is distributed throughout the network based on the specified percentage of grammar 1.

Next, the gamma, age, and cohort attributes of the nodes are initialized.
If gamma is deterministic, the gamma values for all of the nodes will be set to the specified value.
If gamma is probabilistic, the gamma values will be generated using a normal distribution centered around the initial gamma value, with a specified standard deviation.
If gamma is set to be based on age, it will be determined by the "learning curve". 
The learning curve describes the (power law) relationship between age and learning rate, and is as follows:

$y = qc(x^{a})+b$

where $q$ indicates whether the learning rate increases ($+1$) or decreases ($-1$) with age ($x$). 
The constant $c$ shrinks the function to the proper range, and is set to $c = 0.05$. 
The variable $x$ is the normalized age of a given node, such that it is within [0,1].
The variable $a$ represents the "cost" of learning, such that higher "costs" ($a=3$) indicate slow initial change which increases quickly when $x$ is closer to 1, and lower "costs" ($a=0.25$) indicate a quick initial change that slows down later on. 
$b$ is the initial value for gamma.
Graphs for the increasing and decreasing learning curves can be seen below:

```{r gamma-plot, echo = F, fig.height = 3}
gamma.function = function(a, b, age, constant){
    return ( constant * (age^a) + b )
}

## Normal cost curve
a.exp = 1
initial.gamma = 0.005
front.constant = 0.05

normal.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# High cost 
a.exp = 2

high.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# Higher cost 
a.exp = 3

higher.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# Low cost 
a.exp = 0.5

low.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# Lower cost 
a.exp = 0.25

lower.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

df1 <- normal.cost |> 
    rbind(high.cost, higher.cost, low.cost, lower.cost) |> 
    mutate(type = "Increasing (q = 1)")

gamma.function = function(a, b, age, constant){
    return ( constant * (-age^a) + b )
}

## A as cost!!!

## Normal cost curve
a.exp = 1
initial.gamma = 0.055

normal.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# High cost 
a.exp = 2

high.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# Higher cost 
a.exp = 3

higher.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# Low cost 
a.exp = 0.5

low.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

# Lower cost 
a.exp = 0.25

lower.cost <- data.frame(age = seq(0,1,0.01)) |> 
    mutate(gamma = gamma.function(a.exp, initial.gamma, age, front.constant),
           a = a.exp, initial = initial.gamma, constant = front.constant)

df2 <- normal.cost |> 
    rbind(high.cost, higher.cost, low.cost, lower.cost) |> 
    mutate(type = "Decreasing (q = -1)")

df1 |> 
    rbind(df2) |> 
    ggplot(aes(x = age, y = gamma, group = factor(a), color = factor(a)))+
    # geom_point()+
    geom_line(linewidth = 1)+
    theme_bw()+
    xlab("Proportional age (x)")+
    ylab("Gamma parameter")+
    scale_color_brewer(name = "Cost (a)", palette = 5)+
    ggtitle("Learning curve", subtitle = "Relates a proportional age to a gamma value")+
    facet_wrap(~type)

rm(high.cost, higher.cost, low.cost, normal.cost, lower.cost)
```

The number of cohorts is decided by specifying the percentage of Group 2.
The cohorts are initialized such that their ages are either deterministic (set to the specified age) or probabilistic (centered around the specified age, with a specified standard deviation).
When gamma is based on age, any variation in gamma will be determined by the setting of age distribution (deterministic or probabilistic), since they are generated from the learning curve.

## Dynamics

Go procedure:

- Communication

    - Alpha parameter as bias in favor of grammar 1

- Aging

    - Age increases by 1 each tick
    
    - Gamma is modified by the specified parameters
    
Gamma modification:

- Influence is specified at the start: increases, decreases, remains constant

- Every 100 ticks, the gamma is modified by a specified constant, based on the direction

- Minimum gamma is 0; potential for some agents to stop learning




<!-- # ```{r gamma-plot-sigmoid, echo = F} -->
<!-- # gamma.function = function(a, age, constant){ -->
<!-- #     return ( constant / ( 1 + exp(-(2*age - 1)*10*a)  )) -->
<!-- # } -->
<!-- #  -->
<!-- # ## Normal cost curve -->
<!-- # a.exp = 1 -->
<!-- # initial.gamma = 0.05 -->
<!-- # front.constant = 0.05 -->
<!-- #  -->
<!-- # normal.cost <- data.frame(age = seq(0,1,0.01)) |>  -->
<!-- #     mutate(gamma = gamma.function(a.exp, age, front.constant), -->
<!-- #            a = a.exp, initial = initial.gamma, constant = front.constant) -->
<!-- #  -->
<!-- # # High cost  -->
<!-- # a.exp = 2 -->
<!-- # front.constant = 0.05 -->
<!-- #  -->
<!-- # high.cost <- data.frame(age = seq(0,1,0.01)) |>  -->
<!-- #     mutate(gamma = gamma.function(a.exp, age, front.constant), -->
<!-- #            a = a.exp, initial = initial.gamma, constant = front.constant) -->
<!-- #  -->
<!-- # # Higher cost  -->
<!-- # a.exp = 3 -->
<!-- # front.constant = 0.05 -->
<!-- #  -->
<!-- # higher.cost <- data.frame(age = seq(0,1,0.01)) |>  -->
<!-- #     mutate(gamma = gamma.function(a.exp, age, front.constant), -->
<!-- #            a = a.exp, initial = initial.gamma, constant = front.constant) -->
<!-- #  -->
<!-- # # Low cost  -->
<!-- # a.exp = 0.5 -->
<!-- # front.constant = 0.05 -->
<!-- #  -->
<!-- # low.cost <- data.frame(age = seq(0,1,0.01)) |>  -->
<!-- #     mutate(gamma = gamma.function(a.exp, age, front.constant), -->
<!-- #            a = a.exp, initial = initial.gamma, constant = front.constant) -->
<!-- #  -->
<!-- # # Lower cost  -->
<!-- # a.exp = 0.25 -->
<!-- # front.constant = 0.05 -->
<!-- #  -->
<!-- # lower.cost <- data.frame(age = seq(0,1,0.01)) |>  -->
<!-- #     mutate(gamma = gamma.function(a.exp, age, front.constant), -->
<!-- #            a = a.exp, initial = initial.gamma, constant = front.constant) -->
<!-- #  -->
<!-- # normal.cost |>  -->
<!-- #     rbind(high.cost, higher.cost, low.cost, lower.cost) |>  -->
<!-- #     ggplot(aes(x = age, y = gamma, group = a, color = a))+ -->
<!-- #     geom_point()+ -->
<!-- #     geom_line()+ -->
<!-- #     theme_bw()+ -->
<!-- #     xlab("Proportional age")+ -->
<!-- #     ylab("Gamma parameter")+ -->
<!-- #     ggtitle("Learnability parameter changing with age, sigmoid") -->
<!-- #  -->
<!-- # ``` -->

Communication:

- Speaking

    - Original model had variation in whether or not the agents preferred a discrete grammar; this model will take that as granted
    
    - Use a logistic curve, such that agents will produce an utterance that is either 0 or 1.

- Listening

    - Original model had variation in the listening function, either threshold (if neighbors above a threshold value, switch to that grammar), individual (select one neighbor, choose that grammar), or reward (explained below)
    
        - This model will take the third, reward-based algorithm as granted
    
    - Hearing node will pick a grammar that will be used to interpret utterances, either 0 or 1
    
    - If the selected grammar matches the heard grammar, it will update its grammar towards the heard state
    
    - If it fails, it will be updated away from the heard state

# Results

<!-- Result patterns: s-shaped curve, intraspeaker variation, categorical norms, multi-stability, threshold problem -->
<!-- Added: formation of dialect subgroups? -->
<!-- Added: affect of gamma (learning rate), changing gamma with age, two age groups, preference for own group -->

Influence of gamma with only one group:

- Low vs high gamma

```{r gamma-variations, echo = F}
gvdf <- read_csv(here("code",
                      "Language Change Varying Gamma Alpha Percent-table.csv"),
                 show_col_types = F, skip = 6) |> 
    janitor::clean_names()


ggplot(gvdf, aes(x = percent_grammar_1, y = mean_state_of_nodes,
                 fill = factor(initial_gamma),
                 group = factor(initial_gamma)))+
    theme_bw()+
    xlab("Initial percentage of Grammar 1")+
    ylab("Mean state of nodes after 1000 steps")+
    scale_fill_brewer(name = "Initial gamma", type = "qual",
                       palette = 2)+    
    facet_grid(alpha~initial_gamma)+
    stat_summary(fun = mean, geom = "bar")+
    stat_summary(fun.data = mean_se, geom = "errorbar", width = 8)

# no difference?

```

```{r gamma-variations-alt, echo = F}
lndf <- read_csv(here("code",
                      "Language Change test-table.csv"),
                 show_col_types = F, skip = 6) |> 
    janitor::clean_names()

ggplot(lndf, aes(x = step, y = mean_state_of_nodes, group = run_number,
                 color = factor(initial_gamma)))+
    geom_line(alpha = 0.5)+
    scale_color_brewer(name = "Initial gamma", type = "qual" ,
                       palette = 2)+
    theme_bw()+
    facet_grid(percent_grammar_1~initial_gamma)

# Gamma mainly impacts the speed of change

# More variation at gamma = 0.03?

# S-shaped change: Most pronounced with the lowest gamma, change happens most slowly
# Intra speaker variation: All retain a period of inter-speaker variation
# Categorical norms:
# Multi-stability: Higher gammas allow for more stable equilibrium?
# Threshold problem: higher learning lowers the threshold for grammar 1 to become dominant, even when there is not a bias in favor of grammar 1

```

- Probabilistic gamma

```{r gamma-probabilistic-alt, echo = F}
lndf2 <- read_csv(here("code",
                      "Language Change prob-gamma-table.csv"),
                 show_col_types = F, skip = 6) |> 
    janitor::clean_names()

ggplot(lndf2, aes(x = step, y = mean_state_of_nodes, group = run_number,
                 color = factor(initial_gamma)))+
    geom_line(alpha = 0.5)+
    scale_color_brewer(name = "Initial gamma", type = "qual" ,
                       palette = 2)+
    theme_bw()+
    facet_grid(percent_grammar_1~initial_gamma)
```


Aging and gamma:

```{r age-mod-gamma, echo = F}
lndf3 <- read_csv(here("code",
                      "Language Change age-mod-gamma-table.csv"),
                 show_col_types = F, skip = 6) |> 
    janitor::clean_names()

ggplot(lndf3, aes(x = step, y = mean_state_of_nodes, group = run_number,
                  color = factor(percent_grammar_1)))+
    geom_line(alpha = 0.5)+
    scale_color_brewer(name = "Grammar 1\nstarting percentage", type = "qual" ,
                       palette = 3)+
    theme_bw()+
    facet_grid(~percent_grammar_1)
```
- Age varies:

```{r age-vary-mod-gamma, echo = F}
lndf4 <- read_csv(here("code",
                      "Language Change age-vary-mod-gamma-table.csv"),
                 show_col_types = F, skip = 6) |> 
    janitor::clean_names()

ggplot(lndf4, aes(x = step, y = mean_state_of_nodes, group = run_number,
                  color = factor(percent_grammar_1)))+
    geom_line(alpha = 0.5)+
    scale_color_brewer(name = "Grammar 1\nstarting percentage", type = "qual" ,
                       palette = 3)+
    theme_bw()+
    facet_grid(age_sd~percent_grammar_1)


```

Influence of two groups:

```{r two-groups, echo = F}

lndf5 <- read_csv(here("code",
                      "Language Change half-split-age-table.csv"),
                 show_col_types = F, skip = 6) |> 
    janitor::clean_names()

ggplot(lndf5, aes(x = step, y = mean_state_of_nodes, group = run_number,
                  color = factor(percent_grammar_1)))+
    geom_line(alpha = 0.5)+
    scale_color_brewer(name = "Grammar 1\nstarting percentage", type = "qual" ,
                       palette = 3)+
    theme_bw()+
    facet_grid(age_sd~percent_grammar_1)

```


# Discussion

Summarize the results, in the context of previous literature

Comparison with previous model results

Life-long learning, language change

Conclusion.


<!-- Old:  -->
<!-- **a)** For my final project, I will be modifying the model from: Beeksma, M., de Vos, H., Claassen, T., Dijkstra, T., & van Kemenade, A. (2017). A Probabilistic Agent-Based Simulation for Community Level Language Change in Different Scenarios. *Computational Linguistics in the Netherlands Journal*, 7, 17-38. -->

<!-- Their original goal was to simulate historical language change using an agent-based model. They used word order change as a case study, where the model included two potential word orders and two groups, that initially started out with a certain proportion of using the different word orders. They manipulated the learning rate, likelihood of interaction, location-bound dialects, and agent variation within the population. For the outcome, they measured the proportion of the two word orders over time. -->

<!-- Their model consists of two populations of agents that all have individual language models, which influence their interactions. -->
<!-- Each agent has a language model that determines their utterances; when they hear an utterance, it is added to their LM, which they draw from to produce their own utterances. Every step, two agents are selected at random to "interact" (speak and listen), which leads to them updating their language models. -->

<!-- Their main assumptions are as follows (paraphrased from their paper): -->

<!-- 1. Language learning is based on imitating others. -->
<!-- 2. Variability of an individual's language model is limited. -->
<!-- 3. Language can be influenced by external factors (group membership, life expectancy, "norms"). -->

<!-- I aim to explore how potential changes in learning rates across the lifespan may impact overall population-level patterns of language change. Based on the original paper, I have modified the assumptions of the model: -->

<!-- 1. Language learning is based on imitating others, **though this may change over the lifespan and may depend on group membership**. -->
<!-- 2. Variability of an individual's language model is limited. -->
<!-- 3. Language can be influenced by external factors. -->

<!-- Currently, I have their original model running, though the outputted graphs differ slightly. The 1st major step is to **modify the model such that agents "age", which changes their learning/sensitivity parameters.** For example, agents may initially be more willing to learn / sensitive to change, but as they "age", this rate will decrease.  -->
<!-- The 2nd major step is to **modify the cross-interaction (between group interaction) such that learning will be dependent on group preference**. For example, young agents may be more likely to listen and learn from other young agents. -->
<!-- The "stretch goal" is to **modify the model to represent meaning change (instead of word order).** Meaning change will be modeled similar to Kirby 2015, where language has forms {a, b, ...} and meanings {01, 02, 12, ...}. Agents will produce utterances that consist of a form and the intended meaning (i.e. "a01"). -->

<!-- **b)** Similar to the model description above, the system decomposes into language and agents. The language consists of exemplars, which are all the possible utterances; in the original paper there were four possible combinations (two adverb types, two verb types), while the modified version will (potentially) consist of surface forms and meanings (see above). -->

<!-- Each agent has an language model that contains various exemplars, and has an initial preference towards using one type of construction. After my modifications, each agent will also have an age and group preference. As the agents age, their learning rates will decrease based on a specified factor. With group preference, agents may be less likely to listen and learn from out-group members. -->

<!-- Then, at each step, two agents are chosen at random to converse. One agent generates an utterance based on their internal language model, and the other agent hears and integrates that utterance into their language model (dependent on age and group preference). -->

<!-- **c)** The main outcome measure is the proportion of types of utterances over time. I will do batch runs that vary the influence of age and group preference on learning rates. -->
<!-- For the influence of age, "uniform" will be the default, where the learning rate is kept constant regardless of age. Potential variations could be a linear decrease in learning rate as age increases, or maybe a sigmoid function. -->
<!-- For the influence of group preference, I plan to initially keep it as a constant value. I think it will be implemented as out-group learning rate, such that the default will be when learning-rate == out-group learning rate. Then, I can vary instances where the equality varies, such that the out-group learning rate is higher or lower than the overall learning-rate.  -->
<!-- There will be another option, where this group preference (out-group learning rate) changes with age as well. -->

<!-- **d)** I think with the first two goals (age, group preference), it seems like an exciting project looking at the influence of aging on language change. I'm not quite sure about implementing the meaning version of the model (which is why I put it as a stretch goal). For the basic word order version, the outcome measure is similar to the SIR-type models, where the main interest is how a certain type of idea spreads through a system over time. Language change, then, is just the overall prevalence of each type of word order. For the meaning version, though, I would have to implement some "mutation" factor, where the least used surface form mutates its meaning (i.e., "a01" is used least, so it would mutate to "a02"). Then, outcome measure would need to be changed to track both surface form and meaning (something like most frequent meaning per surface form, amount of meaning change per surface form). -->